---
phase: 01-core-metadata
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app.py
autonomous: true

must_haves:
  truths:
    - "User can fetch video statistics via GET /api/statistics/<video_id>"
    - "Response includes view_count, like_count, comment_count, duration (raw, total_seconds, hours, minutes, seconds), definition, caption"
    - "Response includes quota_cost field set to 1"
    - "Duration is parsed from ISO 8601 format (PT#H#M#S) to human-readable components"
    - "Invalid video IDs return 400 with 'Invalid video ID format' error"
    - "Non-existent videos return 404 with 'Video not found' error"
    - "Statistics responses are cached using LRU cache (maxsize 100)"
    - "Endpoint is rate-limited to 10 requests per minute per IP"
  artifacts:
    - path: "app.py"
      provides: "Video statistics fetching via YouTube Data API v3"
      exports: ["get_video_statistics", "parse_duration", "/api/statistics/<video_id>"]
      min_lines: 70
  key_links:
    - from: "app.py"
      to: "isodate.parse_duration"
      via: "isodate library"
      pattern: "isodate\\.parse_duration"
    - from: "parse_duration()"
      to: "get_video_statistics()"
      via: "function call"
      pattern: "parse_duration\\("
    - from: "/api/statistics/<video_id>"
      to: "get_video_statistics()"
      via: "function call"
      pattern: "get_video_statistics\\(video_id\\)"
---

<objective>
Implement video statistics endpoint that returns YouTube video statistics (views, likes, comments, duration, definition, caption status) via `/api/statistics/<video_id>`.

Purpose: Enable API consumers to retrieve video engagement metrics and technical details for analytics, comparison, and display purposes.
Output: Working `/api/statistics/<video_id>` endpoint with LRU caching, rate limiting, ISO 8601 duration parsing, and graceful error handling.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-core-metadata/01-CONTEXT.md
@.planning/phases/01-core-metadata/01-RESEARCH.md
@app.py
</context>

<tasks>

<task type="auto">
  <name>Import isodate library</name>
  <files>app.py</files>
  <action>
    Add `import isodate` to the imports section at the top of app.py. Place it after the `import os` and `import re` lines.

    The isodate library is used to parse YouTube's ISO 8601 duration format (e.g., "PT1H2M3S") into a usable duration object.
  </action>
  <verify>grep -q "import isodate" app.py</verify>
  <done>app.py imports isodate library for duration parsing</done>
</task>

<task type="auto">
  <name>Implement parse_duration helper function</name>
  <files>app.py</files>
  <action>
    Add the `parse_duration()` helper function to app.py, placing it before the route definitions (after the data fetching functions, before @app.route('/'):

    1. Accept iso_duration parameter (string like "PT1H2M3S")
    2. Call isodate.parse_duration(iso_duration) to get duration object
    3. Calculate total_seconds = int(duration.total_seconds())
    4. Extract hours = total_seconds // 3600
    5. Extract minutes = (total_seconds % 3600) // 60
    6. Extract seconds = total_seconds % 60
    7. Return dict with keys: raw (original string), total_seconds, hours, minutes, seconds

    This function must be defined before get_video_statistics() so it can be called from there.
  </action>
  <verify>grep -q "def parse_duration" app.py</verify>
  <done>parse_duration() function exists, converts ISO 8601 duration to dict with raw, total_seconds, hours, minutes, seconds</done>
</task>

<task type="auto">
  <name>Implement get_video_statistics function</name>
  <files>app.py</files>
  <action>
    Add the `get_video_statistics()` function to app.py after the `get_video_metadata()` function (or after `get_video_comments()` if Plan 01 hasn't created get_video_metadata yet):

    1. Use @lru_cache(maxsize=100) decorator for caching
    2. Call youtube.videos().list() with part='statistics,contentDetails' and id=video_id
    3. Check if response['items'] is empty - raise Exception("Video not found") if so
    4. Extract statistics data: viewCount, likeCount, commentCount (convert to int, default to 0)
    5. Extract contentDetails data: duration, definition, caption
    6. Call parse_duration() on duration to get parsed duration object
    7. Return flat dictionary with keys: view_count, like_count, comment_count, duration (parsed dict), definition, caption (boolean: details.get('caption') == 'true')
    8. Handle HttpError for 403 (Access forbidden) and 404 (Video not found) - raise Exception with user-friendly message

    Use the existing error handling pattern from get_video_comments() as reference.
  </action>
  <verify>grep -q "def get_video_statistics" app.py</verify>
  <done>get_video_statistics() function exists, uses YouTube API with part='statistics,contentDetails', parses duration with parse_duration(), returns flat dict with all statistics fields</done>
</task>

<task type="auto">
  <name>Implement /api/statistics/<video_id> endpoint</name>
  <files>app.py</files>
  <action>
    Add the `/api/statistics/<video_id>` route after the `/api/metadata/<video_id>` route (or after `/api/comments/<video_id>` if Plan 01 hasn't run yet):

    1. Use @app.route('/api/statistics/<video_id>', methods=['GET'])
    2. Apply @limiter.limit("10 per minute") decorator
    3. Validate video_id using is_valid_video_id() - return 400 with {'error': 'Invalid video ID format'} if invalid
    4. Call get_video_statistics(video_id) and store result
    5. Return jsonify with: success=True, video_id, quota_cost=1, statistics=<result>
    6. Wrap in try/except handling:
       - "Video not found" -> return 404 with {'error': 'Video not found'}
       - "Access forbidden" -> return 403 with {'error': 'Access forbidden', 'details': 'Quota may be exceeded or video is private'}
       - Other errors -> return 500 with {'error': 'An unexpected error occurred', 'details': <error_message>}

    Follow the exact response pattern from existing transcript/comments endpoints for consistency.
  </action>
  <verify>grep -q "'/api/statistics/<video_id>'" app.py</verify>
  <done>/api/statistics/<video_id> endpoint exists, validates video ID, returns statistics with quota_cost=1, handles all error cases</done>
</task>

</tasks>

<verification>
After completion, verify:
1. `python -c "import app; print('Import successful')"` - no syntax errors
2. Server starts with `python main.py` - no import/dependency errors
3. Manual test: `curl http://localhost:5000/api/statistics/dQw4w9WgXcQ` returns valid JSON with statistics fields including parsed duration
4. Duration parsing test: verify duration object contains raw, total_seconds, hours, minutes, seconds
5. Invalid video ID test: `curl http://localhost:5000/api/statistics/invalid` returns 400
</verification>

<success_criteria>
1. GET /api/statistics/<valid_video_id> returns 200 with success=True, video_id, quota_cost=1, and statistics object
2. Statistics object contains: view_count, like_count, comment_count, duration (with raw, total_seconds, hours, minutes, seconds), definition, caption
3. Duration is correctly parsed from ISO 8601 format (e.g., "PT3M33S" -> {raw: "PT3M33S", total_seconds: 213, hours: 0, minutes: 3, seconds: 33})
4. Invalid video ID format returns 400 error
5. Non-existent video returns 404 error
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-metadata/01-02-SUMMARY.md` with:
- What was implemented (statistics endpoint, parse_duration function, route)
- Any deviations from plan
- Key decisions made (duration format, field names, error handling approach)
- Test results from verification including duration parsing examples
</output>
