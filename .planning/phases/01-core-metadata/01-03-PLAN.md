---
phase: 01-core-metadata
plan: 03
type: execute
wave: 2
depends_on: [01-01, 01-02]
files_modified:
  - app.py
autonomous: true

must_haves:
  truths:
    - "User can fetch complete video data via GET /api/video/<video_id>"
    - "Response includes transcript, metadata, and statistics in single request"
    - "Response includes quota_cost field set to 3 (one for each data type)"
    - "Response includes partial_success flag (false if all data fetched, true if any failures)"
    - "Response includes errors array listing any fetch failures"
    - "All three data types (transcript, metadata, stats) are fetched in parallel using ThreadPoolExecutor"
    - "Invalid video IDs return 400 with 'Invalid video ID format' error"
    - "Complete failures return 500, partial successes return 207 Multi-Status"
    - "Endpoint is rate-limited to 10 requests per minute per IP"
  artifacts:
    - path: "app.py"
      provides: "Unified video data endpoint combining transcript + metadata + statistics"
      exports: ["get_unified_video_data", "/api/video/<video_id>"]
      min_lines: 80
  key_links:
    - from: "get_unified_video_data()"
      to: "ThreadPoolExecutor"
      via: "concurrent.futures module"
      pattern: "ThreadPoolExecutor"
    - from: "get_unified_video_data()"
      to: ["get_transcript()", "get_video_metadata()", "get_video_statistics()"]
      via: "functools.partial"
      pattern: "functools\\.partial"
    - from: "/api/video/<video_id>"
      to: "get_unified_video_data()"
      via: "function call"
      pattern: "get_unified_video_data\\(video_id\\)"
---

<objective>
Implement unified video data endpoint that returns transcript, metadata, and statistics in a single parallel request via `/api/video/<video_id>`.

Purpose: Provide a single endpoint for complete video data, reducing HTTP round-trips and improving response time through parallel fetching.
Output: Working `/api/video/<video_id>` endpoint with parallel data fetching, partial success handling, graceful degradation, and status code differentiation (200 vs 207).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-core-metadata/01-CONTEXT.md
@.planning/phases/01-core-metadata/01-RESEARCH.md
@app.py
@.planning/phases/01-core-metadata/01-01-SUMMARY.md
@.planning/phases/01-core-metadata/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Import concurrent.futures module</name>
  <files>app.py</files>
  <action>
    Add `from concurrent.futures import ThreadPoolExecutor, as_completed` and `import functools` to the imports section at the top of app.py.

    These are needed for parallel execution of the three data fetching functions (get_transcript, get_video_metadata, get_video_statistics).

    Place `from concurrent.futures import ThreadPoolExecutor, as_completed` after the existing `from flask...` imports.
    Place `import functools` after the `import re` line.
  </action>
  <verify>grep -q "from concurrent.futures import ThreadPoolExecutor" app.py && grep -q "import functools" app.py</verify>
  <done>app.py imports ThreadPoolExecutor, as_completed from concurrent.futures, and functools module</done>
</task>

<task type="auto">
  <name>Implement get_unified_video_data function</name>
  <files>app.py</files>
  <action>
    Add the `get_unified_video_data()` function to app.py after the `get_video_statistics()` function:

    1. Use @lru_cache(maxsize=100) decorator for caching
    2. Initialize result dict with: success=True, partial_success=False, video_id (param), quota_cost=3, transcript=None, metadata=None, statistics=None, errors=[]
    3. Create fetch_functions dict with keys 'transcript', 'metadata', 'statistics' mapping to functools.partial(get_*, video_id)
    4. Use ThreadPoolExecutor context manager with max_workers=3:
       - Submit all three fetch functions to executor, storing futures dict
       - Iterate over as_completed(futures), get field_name from futures[future]
       - Try: result[field_name] = future.result()
       - Except: result[field_name] = None; append {'field': field_name, 'error': str(e)} to errors; set partial_success=True
    5. After all futures complete, if errors list not empty: set success = any(transcript, metadata, statistics is not None)
    6. Return result dict

    Important: This function MUST be defined BEFORE the route handler that uses it.
  </action>
  <verify>grep -q "def get_unified_video_data" app.py</verify>
  <done>get_unified_video_data() function exists, uses ThreadPoolExecutor for parallel fetching, returns dict with success, partial_success, quota_cost, transcript/metadata/statistics, and errors array</done>
</task>

<task type="auto">
  <name>Implement /api/video/<video_id> endpoint</name>
  <files>app.py</files>
  <action>
    Add the `/api/video/<video_id>` route after the `/api/statistics/<video_id>` route:

    1. Use @app.route('/api/video/<video_id>', methods=['GET'])
    2. Apply @limiter.limit("10 per minute") decorator
    3. Validate video_id using is_valid_video_id() - return 400 with {'error': 'Invalid video ID format'} if invalid
    4. Call get_unified_video_data(video_id) and store in result variable
    5. Check if result['success'] is False (all fetches failed): return 500 with {'error': 'Failed to retrieve video data', 'details': result['errors']}
    6. Check if result['partial_success'] is True: return jsonify(result) with HTTP status 207 (Multi-Status)
    7. Otherwise (all successful): return jsonify(result) with default 200 status
    8. Wrap entire route in try/except for unexpected errors - return 500 with {'error': 'An unexpected error occurred', 'details': <error_message>}

    Note: Return 207 for partial success (HTTP 207 Multi-Status is appropriate for mixed success/failure responses).
  </action>
  <verify>grep -q "'/api/video/<video_id>'" app.py</verify>
  <done>/api/video/<video_id> endpoint exists, validates video ID, calls get_unified_video_data(), returns 200 for full success, 207 for partial success, 500 for complete failure</done>
</task>

</tasks>

<verification>
After completion, verify:
1. `python -c "import app; print('Import successful')"` - no syntax errors
2. Server starts with `python main.py` - no import/dependency errors
3. Full success test: `curl http://localhost:5000/api/video/dQw4w9WgXcQ` returns 200 with all three data types present and partial_success=false
4. Quota cost verification: response contains quota_cost=3
5. Invalid video ID test: `curl http://localhost:5000/api/video/invalid` returns 400
6. Verify parallel execution: all three data types should be returned, and errors array should be empty for valid videos
</verification>

<success_criteria>
1. GET /api/video/<valid_video_id> returns 200 with success=True, partial_success=False, video_id, quota_cost=3, and non-null transcript/metadata/statistics
2. Transcript field matches what /api/transcript/<id> returns
3. Metadata field matches what /api/metadata/<id> returns
4. Statistics field matches what /api/statistics/<id> returns
5. When one fetch fails (e.g., transcript blocked), response returns 207 with partial_success=True, errors array populated, and other fields still populated
6. When all fetches fail, response returns 500 with error message
7. Response time is faster than sequential fetching (due to parallel execution)
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-metadata/01-03-SUMMARY.md` with:
- What was implemented (unified endpoint with parallel fetching)
- Any deviations from plan
- Key decisions made (status code 207 for partial success, max_workers=3, error aggregation)
- Test results from verification including timing comparison vs sequential requests
- Final phase summary noting Phase 1 success criteria met
</output>
